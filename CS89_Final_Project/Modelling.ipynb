{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3adb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: This notebook requires the installation of GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af7d31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "from scipy.sparse import vstack, hstack\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77b91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_file_path = './Text_Scraping/speeches_df_2000_to_2023.csv'\n",
    "\n",
    "speeches_df = pd.read_csv(speeches_file_path)\n",
    "speeches_df['Date'] = pd.to_datetime(speeches_df['Date'])\n",
    "speeches_df = speeches_df.drop(['Unnamed: 0', 'quarter', 'authorId', 'year'], axis=1)\n",
    "\n",
    "types = ['Speech'] * speeches_df.shape[0]\n",
    "speeches_df['Type'] = types\n",
    "\n",
    "meetings_path = './Text_Scraping/FOMC_Statements_and_Minutes.csv'\n",
    "meetings_df = pd.read_csv(meetings_path)\n",
    "meetings_df['Date'] = pd.to_datetime(meetings_df['Date'])\n",
    "meetings_df = meetings_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "authors = []\n",
    "for row in meetings_df['Type']:\n",
    "    if row == 'Minutes':\n",
    "        authors.append('FOMC Minutes')\n",
    "    elif row == 'Statement':\n",
    "        authors.append('FOMC Statement')\n",
    "    else:\n",
    "        authors.append('None')\n",
    "\n",
    "meetings_df['Author'] = authors\n",
    "\n",
    "text_df = result_df = pd.concat([speeches_df, meetings_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b44476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>2_YR_Treasury</th>\n",
       "      <th>Eff_Fed_Funds</th>\n",
       "      <th>Real_GDP</th>\n",
       "      <th>Core_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>UnitedHealth_Group_Inc sentiment</th>\n",
       "      <th>Cisco_Systems_Inc sentiment</th>\n",
       "      <th>Chevron_Corp sentiment</th>\n",
       "      <th>Microsoft_Corp sentiment</th>\n",
       "      <th>CocaCola_Co sentiment</th>\n",
       "      <th>Amgen_Inc sentiment</th>\n",
       "      <th>Caterpillar_Inc sentiment</th>\n",
       "      <th>Procter__Gamble_Co sentiment</th>\n",
       "      <th>Walmart_Inc sentiment</th>\n",
       "      <th>Merck__Co_Inc sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-12-08</td>\n",
       "      <td>Chairman Alan Greenspan</td>\n",
       "      <td>\\n \\n\\r\\nBuildings such as this new Birmingha...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.50</td>\n",
       "      <td>6.47</td>\n",
       "      <td>13262.250</td>\n",
       "      <td>2.983840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146199</td>\n",
       "      <td>-0.361702</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>-0.606383</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>-0.170984</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-12-06</td>\n",
       "      <td>Vice Chairman Roger W. Ferguson, Jr.</td>\n",
       "      <td>\\n \\n\\n\\r\\n\\tThank you for inviting me to the...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.42</td>\n",
       "      <td>6.48</td>\n",
       "      <td>13262.250</td>\n",
       "      <td>2.983840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146199</td>\n",
       "      <td>-0.361702</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>-0.606383</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>-0.170984</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-12-06</td>\n",
       "      <td>Governor Edward M. Gramlich</td>\n",
       "      <td>\\n \\n\\n Subprime Lending, Predatory Lending \\...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.42</td>\n",
       "      <td>6.48</td>\n",
       "      <td>13262.250</td>\n",
       "      <td>2.983840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146199</td>\n",
       "      <td>-0.361702</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>-0.606383</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>-0.170984</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-12-05</td>\n",
       "      <td>Chairman Alan Greenspan</td>\n",
       "      <td>\\n \\n\\r\\nTechnological innovation, and in par...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.49</td>\n",
       "      <td>6.51</td>\n",
       "      <td>13262.250</td>\n",
       "      <td>2.983840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146199</td>\n",
       "      <td>-0.361702</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>-0.606383</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>-0.170984</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-11-21</td>\n",
       "      <td>Governor Edward M. Gramlich</td>\n",
       "      <td>\\n \\n\\n Financial Literacy \\n \\r\\nPartnership...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.86</td>\n",
       "      <td>6.50</td>\n",
       "      <td>13262.250</td>\n",
       "      <td>2.881613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146199</td>\n",
       "      <td>-0.361702</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>-0.606383</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>-0.170984</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>2010-05-09</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>\\nThe Federal Reserve, the central bank of the...</td>\n",
       "      <td>Statement</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>15605.628</td>\n",
       "      <td>0.799868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364532</td>\n",
       "      <td>-0.449876</td>\n",
       "      <td>-0.509537</td>\n",
       "      <td>-0.441935</td>\n",
       "      <td>-0.416938</td>\n",
       "      <td>-0.380835</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.267057</td>\n",
       "      <td>-0.551020</td>\n",
       "      <td>-0.794949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>\\nThe Federal Reserve, the central bank of the...</td>\n",
       "      <td>Statement</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.59</td>\n",
       "      <td>18951.992</td>\n",
       "      <td>2.641068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318504</td>\n",
       "      <td>-0.549211</td>\n",
       "      <td>-0.426654</td>\n",
       "      <td>-0.481121</td>\n",
       "      <td>-0.273030</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-0.362930</td>\n",
       "      <td>-0.138095</td>\n",
       "      <td>-0.440520</td>\n",
       "      <td>-0.273951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>\\nThe Federal Reserve, the central bank of the...</td>\n",
       "      <td>Statement</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>18951.992</td>\n",
       "      <td>2.641068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318504</td>\n",
       "      <td>-0.549211</td>\n",
       "      <td>-0.426654</td>\n",
       "      <td>-0.481121</td>\n",
       "      <td>-0.273030</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-0.362930</td>\n",
       "      <td>-0.138095</td>\n",
       "      <td>-0.440520</td>\n",
       "      <td>-0.273951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>\\nThe Federal Reserve, the central bank of the...</td>\n",
       "      <td>Statement</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>18951.992</td>\n",
       "      <td>2.641068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.410000</td>\n",
       "      <td>-0.549211</td>\n",
       "      <td>-0.558648</td>\n",
       "      <td>-0.490241</td>\n",
       "      <td>-0.341693</td>\n",
       "      <td>-0.425390</td>\n",
       "      <td>-0.362930</td>\n",
       "      <td>-0.360419</td>\n",
       "      <td>-0.440520</td>\n",
       "      <td>-0.424149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>\\nThe Federal Reserve, the central bank of the...</td>\n",
       "      <td>Statement</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.82</td>\n",
       "      <td>19202.310</td>\n",
       "      <td>2.734200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324324</td>\n",
       "      <td>-0.455891</td>\n",
       "      <td>-0.508333</td>\n",
       "      <td>-0.503759</td>\n",
       "      <td>-0.345912</td>\n",
       "      <td>-0.426667</td>\n",
       "      <td>-0.362930</td>\n",
       "      <td>-0.183544</td>\n",
       "      <td>-0.471264</td>\n",
       "      <td>-0.370031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1921 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date                                Author  \\\n",
       "0    2000-12-08               Chairman Alan Greenspan   \n",
       "1    2000-12-06  Vice Chairman Roger W. Ferguson, Jr.   \n",
       "2    2000-12-06           Governor Edward M. Gramlich   \n",
       "3    2000-12-05               Chairman Alan Greenspan   \n",
       "4    2000-11-21           Governor Edward M. Gramlich   \n",
       "...         ...                                   ...   \n",
       "1916 2010-05-09                        FOMC Statement   \n",
       "1917 2020-03-03                        FOMC Statement   \n",
       "1918 2020-03-23                        FOMC Statement   \n",
       "1919 2020-03-31                        FOMC Statement   \n",
       "1920 2019-10-11                        FOMC Statement   \n",
       "\n",
       "                                                   Text       Type    year  \\\n",
       "0      \\n \\n\\r\\nBuildings such as this new Birmingha...     Speech     NaN   \n",
       "1      \\n \\n\\n\\r\\n\\tThank you for inviting me to the...     Speech     NaN   \n",
       "2      \\n \\n\\n Subprime Lending, Predatory Lending \\...     Speech     NaN   \n",
       "3      \\n \\n\\r\\nTechnological innovation, and in par...     Speech     NaN   \n",
       "4      \\n \\n\\n Financial Literacy \\n \\r\\nPartnership...     Speech     NaN   \n",
       "...                                                 ...        ...     ...   \n",
       "1916  \\nThe Federal Reserve, the central bank of the...  Statement  2010.0   \n",
       "1917  \\nThe Federal Reserve, the central bank of the...  Statement  2020.0   \n",
       "1918  \\nThe Federal Reserve, the central bank of the...  Statement  2020.0   \n",
       "1919  \\nThe Federal Reserve, the central bank of the...  Statement  2020.0   \n",
       "1920  \\nThe Federal Reserve, the central bank of the...  Statement  2019.0   \n",
       "\n",
       "      quarter  2_YR_Treasury  Eff_Fed_Funds   Real_GDP  Core_CPI  ...  \\\n",
       "0         NaN           5.50           6.47  13262.250  2.983840  ...   \n",
       "1         NaN           5.42           6.48  13262.250  2.983840  ...   \n",
       "2         NaN           5.42           6.48  13262.250  2.983840  ...   \n",
       "3         NaN           5.49           6.51  13262.250  2.983840  ...   \n",
       "4         NaN           5.86           6.50  13262.250  2.881613  ...   \n",
       "...       ...            ...            ...        ...       ...  ...   \n",
       "1916      2.0           0.83           0.20  15605.628  0.799868  ...   \n",
       "1917      1.0           0.71           1.59  18951.992  2.641068  ...   \n",
       "1918      1.0           0.28           0.15  18951.992  2.641068  ...   \n",
       "1919      1.0           0.23           0.08  18951.992  2.641068  ...   \n",
       "1920      4.0           1.63           1.82  19202.310  2.734200  ...   \n",
       "\n",
       "      UnitedHealth_Group_Inc sentiment  Cisco_Systems_Inc sentiment  \\\n",
       "0                            -0.146199                    -0.361702   \n",
       "1                            -0.146199                    -0.361702   \n",
       "2                            -0.146199                    -0.361702   \n",
       "3                            -0.146199                    -0.361702   \n",
       "4                            -0.146199                    -0.361702   \n",
       "...                                ...                          ...   \n",
       "1916                         -0.364532                    -0.449876   \n",
       "1917                         -0.318504                    -0.549211   \n",
       "1918                         -0.318504                    -0.549211   \n",
       "1919                         -0.410000                    -0.549211   \n",
       "1920                         -0.324324                    -0.455891   \n",
       "\n",
       "      Chevron_Corp sentiment  Microsoft_Corp sentiment  CocaCola_Co sentiment  \\\n",
       "0                  -0.348485                 -0.606383               0.084746   \n",
       "1                  -0.348485                 -0.606383               0.084746   \n",
       "2                  -0.348485                 -0.606383               0.084746   \n",
       "3                  -0.348485                 -0.606383               0.084746   \n",
       "4                  -0.348485                 -0.606383               0.084746   \n",
       "...                      ...                       ...                    ...   \n",
       "1916               -0.509537                 -0.441935              -0.416938   \n",
       "1917               -0.426654                 -0.481121              -0.273030   \n",
       "1918               -0.426654                 -0.481121              -0.273030   \n",
       "1919               -0.558648                 -0.490241              -0.341693   \n",
       "1920               -0.508333                 -0.503759              -0.345912   \n",
       "\n",
       "      Amgen_Inc sentiment  Caterpillar_Inc sentiment  \\\n",
       "0               -0.212121                  -0.170984   \n",
       "1               -0.212121                  -0.170984   \n",
       "2               -0.212121                  -0.170984   \n",
       "3               -0.212121                  -0.170984   \n",
       "4               -0.212121                  -0.170984   \n",
       "...                   ...                        ...   \n",
       "1916            -0.380835                  -0.500000   \n",
       "1917            -0.888889                  -0.362930   \n",
       "1918            -0.888889                  -0.362930   \n",
       "1919            -0.425390                  -0.362930   \n",
       "1920            -0.426667                  -0.362930   \n",
       "\n",
       "      Procter__Gamble_Co sentiment  Walmart_Inc sentiment  \\\n",
       "0                        -0.292929              -0.076923   \n",
       "1                        -0.292929              -0.076923   \n",
       "2                        -0.292929              -0.076923   \n",
       "3                        -0.292929              -0.076923   \n",
       "4                        -0.292929              -0.076923   \n",
       "...                            ...                    ...   \n",
       "1916                     -0.267057              -0.551020   \n",
       "1917                     -0.138095              -0.440520   \n",
       "1918                     -0.138095              -0.440520   \n",
       "1919                     -0.360419              -0.440520   \n",
       "1920                     -0.183544              -0.471264   \n",
       "\n",
       "      Merck__Co_Inc sentiment  \n",
       "0                   -1.000000  \n",
       "1                   -1.000000  \n",
       "2                   -1.000000  \n",
       "3                   -1.000000  \n",
       "4                   -1.000000  \n",
       "...                       ...  \n",
       "1916                -0.794949  \n",
       "1917                -0.273951  \n",
       "1918                -0.273951  \n",
       "1919                -0.424149  \n",
       "1920                -0.370031  \n",
       "\n",
       "[1921 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_path = 'Macro-Micro/macro_indicators.csv'\n",
    "\n",
    "macro_df = pd.read_csv(macro_path)\n",
    "macro_df['Date'] = macro_df['Unnamed: 0']\n",
    "macro_df['Date'] = pd.to_datetime(macro_df['Date'])\n",
    "df = pd.merge(text_df, macro_df, on='Date', how='inner')\n",
    "\n",
    "directory = 'Macro-Micro/company_sentiment_count/'\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename == 'JPMorgan_Chase__Co.csv' or filename == 'Goldman_Sachs_Group_Inc.csv':\n",
    "        continue\n",
    "    f = os.path.join(directory, filename)\n",
    "    \n",
    "    # Read company sentiment DataFrame\n",
    "    company_df = pd.read_csv(f)\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    company_df[filename.split('.')[0] + ' sentiment'] = (company_df['positive'] - company_df['negative']) / (company_df['positive'] + company_df['negative'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Rename 'date' column to 'Date'\n",
    "    company_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    company_df['Date'] = pd.to_datetime(company_df['Date'])\n",
    "    \n",
    "    company_df = company_df[~company_df['Date'].duplicated(keep='first')]\n",
    "    \n",
    "    company_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    \n",
    "    # Resample the DataFrame to include every day\n",
    "    df_resampled = company_df.resample('D').asfreq()\n",
    "\n",
    "    # Sort the DataFrame by the \"Date\" column\n",
    "    df_resampled.sort_values(by='Date', inplace=True)\n",
    "\n",
    "    # Forward fill missing values in all columns\n",
    "    df_resampled.ffill(inplace=True)\n",
    "\n",
    "    # Reset the index to make \"Date\" a column again\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    \n",
    "    company_df = df_resampled\n",
    "    \n",
    "    company_df[filename.split('.')[0] + ' sentiment'] = company_df[filename.split('.')[0] + ' sentiment'].fillna(method='ffill')\n",
    "    \n",
    "    sentiment_col = []\n",
    "    for date in df['Date']:\n",
    "        if date in company_df['Date'].values:\n",
    "            cell_value = company_df.loc[company_df['Date'] == date, filename.split('.')[0] + ' sentiment'].iloc[0]\n",
    "            sentiment_col.append(cell_value)\n",
    "        else:\n",
    "            sentiment_col.append(0)\n",
    "    \n",
    "    df[filename.split('.')[0] + ' sentiment'] = sentiment_col\n",
    "\n",
    "df=df.drop(['Unnamed: 0'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301495fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting PR Column\n",
    "delay = datetime.timedelta(days=60)\n",
    "\n",
    "future_change = []\n",
    "\n",
    "rate_classification = []\n",
    "\n",
    "curr_rates = []\n",
    "\n",
    "future_rates = []\n",
    "\n",
    "for date in df['Date']:\n",
    "    data = web.DataReader(['FEDFUNDS'], 'fred', date, date + delay)\n",
    "    if len(data['FEDFUNDS']) < 2:\n",
    "        \n",
    "        future_change.append(np.nan)\n",
    "        rate_classification.append(np.nan)\n",
    "        future_rates.append(np.nan)\n",
    "        curr_rates.append(np.nan)\n",
    "    else:\n",
    "        future_change.append(data['FEDFUNDS'][-1] - data['FEDFUNDS'][0])\n",
    "        curr_rates.append(data['FEDFUNDS'][0])\n",
    "        future_rates.append(data['FEDFUNDS'][-1])\n",
    "        \n",
    "        if data['FEDFUNDS'][-1] > data['FEDFUNDS'][0]:\n",
    "            rate_classification.append(1)\n",
    "        elif data['FEDFUNDS'][-1] == data['FEDFUNDS'][0]:\n",
    "            rate_classification.append(0)\n",
    "        else:\n",
    "            rate_classification.append(-1)\n",
    "\n",
    "df['PR'] = future_change\n",
    "df['Class'] = rate_classification\n",
    "df['Future'] = future_rates\n",
    "df['Curr'] = curr_rates\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e30ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_features(X_train, X_test, train_model, vector_size):\n",
    "\n",
    "    X_train = [nltk.word_tokenize(row) for row in X_train]\n",
    "    X_test = [nltk.word_tokenize(row) for row in X_test]\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_embeddings = []\n",
    "    for row in X_train:\n",
    "        embedding = []\n",
    "        zeros = True\n",
    "        for word in row:\n",
    "            if word in train_model.wv:\n",
    "                embedding.append(train_model.wv[word])\n",
    "                zeros = False\n",
    "                \n",
    "        if zeros:\n",
    "            train_embeddings.append(np.zeros(vector_size))\n",
    "        else:\n",
    "            train_embeddings.append(np.mean(embedding, axis=0))\n",
    "            \n",
    "    X_train_average = train_embeddings\n",
    "    \n",
    "    test_embeddings = []\n",
    "    for row in X_test:\n",
    "        embedding = []\n",
    "        zeros = True\n",
    "        for word in row:\n",
    "            if word in train_model.wv:\n",
    "                embedding.append(train_model.wv[word])\n",
    "                zeros = False\n",
    "                \n",
    "        if zeros:\n",
    "            test_embeddings.append(np.zeros(vector_size))\n",
    "        else:\n",
    "            test_embeddings.append(np.mean(embedding, axis=0))\n",
    "            \n",
    "    X_test_average = test_embeddings\n",
    "    \n",
    "    return X_train_average, X_test_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e05090ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_text = df['Text']\n",
    "X_text_dates = df[['Date', 'Type']]\n",
    "X_macro = df[['2_YR_Treasury', 'Eff_Fed_Funds',\n",
    "       'Real_GDP', 'Core_CPI', 'PCE', 'Unemployment', 'Savings_Rate',\n",
    "       'Retail_Sales', 'Manufacturing_PMI', 'Consumer_Sent', 'Liquidity',\n",
    "       'Volatility', 'SP_500', '2_YR_Treasury_pct_chng',\n",
    "       'Eff_Fed_Funds_pct_chng', 'Real_GDP_pct_chng', 'Core_CPI_pct_chng',\n",
    "       'PCE_pct_chng', 'Unemployment_pct_chng', 'Savings_Rate_pct_chng',\n",
    "       'Retail_Sales_pct_chng', 'Manufacturing_PMI_pct_chng',\n",
    "       'Consumer_Sent_pct_chng', 'Liquidity_pct_chng', 'Volatility_pct_chng',\n",
    "       'SP_500_pct_chng']]\n",
    "\n",
    "X_micro = df[['Honeywell_International_Inc sentiment',\n",
    "       'Travelers_Companies_Inc sentiment', 'Boeing_Co sentiment',\n",
    "       'American_Express_Co sentiment', 'Nike_Inc sentiment',\n",
    "       'Walt_Disney_Co sentiment', 'Intel_Corp sentiment',\n",
    "       'Johnson__Johnson sentiment', 'McDonalds_Corp sentiment',\n",
    "       'International_Business_Machines_Corp sentiment',\n",
    "       'Walgreens_Boots_Alliance_Inc sentiment', '3M_Co sentiment',\n",
    "       'Visa_Inc sentiment', 'Apple_Inc sentiment',\n",
    "       'Verizon_Communications_Inc sentiment', 'Dow_Inc sentiment',\n",
    "       'Salesforce_Inc sentiment', 'Home_Depot_Inc sentiment',\n",
    "       'UnitedHealth_Group_Inc sentiment', 'Cisco_Systems_Inc sentiment',\n",
    "       'Chevron_Corp sentiment', 'Microsoft_Corp sentiment',\n",
    "       'CocaCola_Co sentiment', 'Amgen_Inc sentiment',\n",
    "       'Caterpillar_Inc sentiment', 'Procter__Gamble_Co sentiment',\n",
    "       'Walmart_Inc sentiment', 'Merck__Co_Inc sentiment']]\n",
    "\n",
    "X_auto = df['Curr']\n",
    "\n",
    "\n",
    "y_reg = df['PR']\n",
    "\n",
    "y_class = df['Class']\n",
    "\n",
    "\n",
    "X_text_train, X_text_test, X_train_macro, X_test_macro, X_train_micro, X_test_micro, X_auto_train, X_auto_test, y_reg_train, y_reg_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_text, X_macro, X_micro, X_auto, y_reg, y_class, test_size=0.2\n",
    ")\n",
    "\n",
    "# vectorizer the text\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(1, 2), stop_words='english', max_df=0.7, min_df=5)\n",
    "X_text_train_tfidf = tfidf_vectorizer.fit_transform(X_text_train)\n",
    "X_text_test_tfidf = tfidf_vectorizer.transform(X_text_test)\n",
    "\n",
    "# scaling for macro model\n",
    "scaler = StandardScaler()\n",
    "X_train_macro.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_train_macro.fillna(0, inplace=True)\n",
    "X_test_macro.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test_macro.fillna(0, inplace=True)\n",
    "X_train_macro = scaler.fit_transform(X_train_macro)\n",
    "X_test_macro = scaler.transform(X_test_macro)\n",
    "\n",
    "# semantic model\n",
    "google_model_key = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=40000)\n",
    "google_model = gensim.models.Word2Vec()\n",
    "google_model.wv = google_model_key\n",
    "X_train_google, X_test_google = get_semantic_features(X_text_train, X_text_test, google_model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04dbc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto model\n",
    "auto_reg = LinearRegression()\n",
    "auto_reg.fit(np.array(X_auto_train).reshape(-1, 1), y_reg_train)\n",
    "auto_reg_pred = auto_reg.predict(np.array(X_auto_test).reshape(-1, 1))\n",
    "\n",
    "# text only regression\n",
    "text_reg_model = LinearRegression()\n",
    "text_reg_model.fit(X_text_train_tfidf, y_reg_train)\n",
    "y_pred_text_reg = text_reg_model.predict(X_text_test_tfidf)\n",
    "\n",
    "# macro only regression\n",
    "macro_reg_model = LinearRegression()\n",
    "macro_reg_model.fit(X_train_macro, y_reg_train)\n",
    "y_pred_macro_reg = macro_reg_model.predict(X_test_macro)\n",
    "\n",
    "# micro only regression\n",
    "micro_reg_model = LinearRegression()\n",
    "micro_reg_model.fit(X_train_micro, y_reg_train)\n",
    "y_pred_micro_reg = micro_reg_model.predict(X_test_micro)\n",
    "\n",
    "# semantic regression\n",
    "semantic_reg_model = LinearRegression()\n",
    "semantic_reg_model.fit(X_train_google, y_reg_train)\n",
    "y_pred_semantic_reg = semantic_reg_model.predict(X_test_google)\n",
    "\n",
    "# dummy model\n",
    "dummy=DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_text_train_tfidf, y_class_train)\n",
    "dummy_pred = dummy.predict(X_text_test_tfidf)\n",
    "\n",
    "# text only class\n",
    "text_class_model = LogisticRegression(max_iter = 1000)\n",
    "text_class_model.fit(X_text_train_tfidf, y_class_train)\n",
    "y_pred_text_class = text_class_model.predict(X_text_test_tfidf)\n",
    "\n",
    "# numerical only class\n",
    "macro_class_model = LogisticRegression(max_iter = 1000)\n",
    "macro_class_model.fit(X_train_macro, y_class_train)\n",
    "y_pred_macro_class = macro_class_model.predict(X_test_macro)\n",
    "\n",
    "# micro only class\n",
    "micro_class_model = LogisticRegression(max_iter = 1000)\n",
    "micro_class_model.fit(X_train_micro, y_class_train)\n",
    "y_pred_micro_class = micro_class_model.predict(X_test_micro)\n",
    "\n",
    "# semantic class\n",
    "semantic_class_model = LogisticRegression(max_iter = 1000)\n",
    "semantic_class_model.fit(X_train_google, y_class_train)\n",
    "y_pred_semantic_class = semantic_class_model.predict(X_test_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "233ecf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Regression Models-------\n",
      "Auto MSE: 0.05827756565176713\n",
      "Text MSE: 0.030600491775177367\n",
      "Macro MSE: 0.03442092023180792\n",
      "Micro MSE: 0.04198849484010601\n",
      "Semantic MSE: 1.8129433880015189\n",
      "-------Classification Models-------\n",
      "Auto F1: 0.26074925500212853\n",
      "Text F1: 0.5961327155419982\n",
      "Macro F1: 0.5688358828104745\n",
      "Micro F1: 0.6100823045267488\n",
      "Semantic F1: 0.39477332978381197\n"
     ]
    }
   ],
   "source": [
    "print('-------Regression Models-------')\n",
    "auto_mse = mean_squared_error(y_reg_test, auto_reg_pred)\n",
    "print(\"Auto MSE:\", auto_mse)\n",
    "\n",
    "text_only_mse = mean_squared_error(y_reg_test, y_pred_text_reg)\n",
    "print(\"Text MSE:\", text_only_mse)\n",
    "\n",
    "macro_only_mse = mean_squared_error(y_reg_test, y_pred_macro_reg)\n",
    "print(\"Macro MSE:\", macro_only_mse)\n",
    "\n",
    "micro_only_mse = mean_squared_error(y_reg_test, y_pred_micro_reg)\n",
    "print(\"Micro MSE:\", micro_only_mse)\n",
    "\n",
    "semantic_only_mse = mean_squared_error(y_reg_test, y_pred_semantic_reg)\n",
    "print(\"Semantic MSE:\", semantic_only_mse)\n",
    "\n",
    "print('-------Classification Models-------')\n",
    "auto_f1 = f1_score(y_class_test, dummy_pred, average='weighted')\n",
    "print(\"Auto F1:\", auto_f1)\n",
    "\n",
    "text_only_f1 = f1_score(y_class_test, y_pred_text_class, average='weighted')\n",
    "print(\"Text F1:\", text_only_f1)\n",
    "\n",
    "macro_only_f1 = f1_score(y_class_test, y_pred_macro_class, average='weighted')\n",
    "print(\"Macro F1:\", macro_only_f1)\n",
    "\n",
    "micro_only_f1 = f1_score(y_class_test, y_pred_micro_class, average='weighted')\n",
    "print(\"Micro F1:\", micro_only_f1)\n",
    "\n",
    "semantic_only_f1 = f1_score(y_class_test, y_pred_semantic_class, average='weighted')\n",
    "print(\"Semantic F1:\", semantic_only_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cf77c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0.2559737351175463, 0.33362534537271754, 0.3108413127732934, 0.45146341463414635]\n",
      "[0, 0, 1, 0, 0.030241093127397516, 0.11952334603721522, 0.5371242135924592, 0.5811585365853658]\n",
      "[0, 0, 1, 1, 0.43129079368945755, 0.46511571348127745, 0.548242721132067, 0.5911585365853658]\n",
      "[0, 1, 0, 0, 0.021586576310486046, 0.0990380116416582, 0.5889299142908546, 0.6008536585365853]\n",
      "[0, 1, 0, 1, 0.4712372334534862, 0.446133852374966, 0.6014938941654606, 0.6134146341463416]\n",
      "[0, 1, 1, 0, 0.020058930259986764, 0.09741105039244033, 0.6381426984842082, 0.6507926829268293]\n",
      "[0, 1, 1, 1, 10.79010118685279, 1.7580724672059453, 0.6533158037180492, 0.6607317073170731]\n",
      "[1, 0, 0, 0, 0.02030545676412905, 0.08301857484801314, 0.5833399862077776, 0.6008536585365853]\n",
      "[1, 0, 0, 1, 0.020285679111794026, 0.08270891558552801, 0.5882743101950539, 0.6059756097560974]\n",
      "[1, 0, 1, 0, 0.018633851311659462, 0.08356535143237268, 0.6039012860784683, 0.6309756097560976]\n",
      "[1, 0, 1, 1, 0.01856184501410352, 0.08318498110276012, 0.6047895976694114, 0.6284756097560976]\n",
      "[1, 1, 0, 0, 0.019125008056162997, 0.08582148433932493, 0.6094956261980257, 0.6184146341463415]\n",
      "[1, 1, 0, 1, 0.019087046614204472, 0.08581193564614742, 0.6082355451014281, 0.6159146341463415]\n",
      "[1, 1, 1, 0, 0.018646847313740562, 0.08700859687167636, 0.6467758041859, 0.6558536585365854]\n",
      "[1, 1, 1, 1, 0.018564617541969912, 0.08679390856593738, 0.6483054430091028, 0.6558536585365853]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>macro</th>\n",
       "      <th>micro</th>\n",
       "      <th>semantic</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.255974</td>\n",
       "      <td>0.333625</td>\n",
       "      <td>0.310841</td>\n",
       "      <td>0.451463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030241</td>\n",
       "      <td>0.119523</td>\n",
       "      <td>0.537124</td>\n",
       "      <td>0.581159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.431291</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.548243</td>\n",
       "      <td>0.591159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021587</td>\n",
       "      <td>0.099038</td>\n",
       "      <td>0.588930</td>\n",
       "      <td>0.600854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471237</td>\n",
       "      <td>0.446134</td>\n",
       "      <td>0.601494</td>\n",
       "      <td>0.613415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.097411</td>\n",
       "      <td>0.638143</td>\n",
       "      <td>0.650793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.790101</td>\n",
       "      <td>1.758072</td>\n",
       "      <td>0.653316</td>\n",
       "      <td>0.660732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>0.083019</td>\n",
       "      <td>0.583340</td>\n",
       "      <td>0.600854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020286</td>\n",
       "      <td>0.082709</td>\n",
       "      <td>0.588274</td>\n",
       "      <td>0.605976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.083565</td>\n",
       "      <td>0.603901</td>\n",
       "      <td>0.630976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>0.083185</td>\n",
       "      <td>0.604790</td>\n",
       "      <td>0.628476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019125</td>\n",
       "      <td>0.085821</td>\n",
       "      <td>0.609496</td>\n",
       "      <td>0.618415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019087</td>\n",
       "      <td>0.085812</td>\n",
       "      <td>0.608236</td>\n",
       "      <td>0.615915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.087009</td>\n",
       "      <td>0.646776</td>\n",
       "      <td>0.655854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>0.086794</td>\n",
       "      <td>0.648305</td>\n",
       "      <td>0.655854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tfidf  macro  micro  semantic        mse       mae        f1  accuracy\n",
       "0     0.0    0.0    0.0       1.0   0.255974  0.333625  0.310841  0.451463\n",
       "1     0.0    0.0    1.0       0.0   0.030241  0.119523  0.537124  0.581159\n",
       "2     0.0    0.0    1.0       1.0   0.431291  0.465116  0.548243  0.591159\n",
       "3     0.0    1.0    0.0       0.0   0.021587  0.099038  0.588930  0.600854\n",
       "4     0.0    1.0    0.0       1.0   0.471237  0.446134  0.601494  0.613415\n",
       "5     0.0    1.0    1.0       0.0   0.020059  0.097411  0.638143  0.650793\n",
       "6     0.0    1.0    1.0       1.0  10.790101  1.758072  0.653316  0.660732\n",
       "7     1.0    0.0    0.0       0.0   0.020305  0.083019  0.583340  0.600854\n",
       "8     1.0    0.0    0.0       1.0   0.020286  0.082709  0.588274  0.605976\n",
       "9     1.0    0.0    1.0       0.0   0.018634  0.083565  0.603901  0.630976\n",
       "10    1.0    0.0    1.0       1.0   0.018562  0.083185  0.604790  0.628476\n",
       "11    1.0    1.0    0.0       0.0   0.019125  0.085821  0.609496  0.618415\n",
       "12    1.0    1.0    0.0       1.0   0.019087  0.085812  0.608236  0.615915\n",
       "13    1.0    1.0    1.0       0.0   0.018647  0.087009  0.646776  0.655854\n",
       "14    1.0    1.0    1.0       1.0   0.018565  0.086794  0.648305  0.655854"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_tfidf = vstack([X_text_train_tfidf, X_text_test_tfidf] )\n",
    "X_macro = np.vstack([X_train_macro, X_test_macro])\n",
    "X_micro = pd.concat([X_train_micro, X_test_micro], axis=0)\n",
    "X_google = np.vstack([X_train_google, X_test_google])\n",
    "y_reg = pd.concat([y_reg_train, y_reg_test], axis = 0)\n",
    "y_class = pd.concat([y_class_train, y_class_test], axis = 0)\n",
    "\n",
    "result_df = pd.DataFrame(columns = ['tfidf', 'macro', 'micro', 'semantic', 'mse', 'mae', 'f1', 'accuracy'])\n",
    "\n",
    "for tfidf in range(2):\n",
    "    for macro in range(2):\n",
    "        for micro in range(2):\n",
    "            for semantic in range(2):\n",
    "                \n",
    "                \n",
    "                if not (macro or micro or tfidf or semantic):\n",
    "                    continue\n",
    "                \n",
    "                all_features = None\n",
    "                \n",
    "                if tfidf:\n",
    "                    text_features = X_text_tfidf \n",
    "                    if macro:\n",
    "                        text_features = hstack([text_features, X_macro])\n",
    "                    if micro:\n",
    "                        text_features = hstack([text_features, X_micro])\n",
    "                    if semantic:\n",
    "                        text_features = hstack([text_features, X_google])\n",
    "                    all_features = text_features\n",
    "                else:\n",
    "                    non_text_features = pd.DataFrame()\n",
    "                    if macro:\n",
    "                        non_text_features = pd.concat([non_text_features, pd.DataFrame(X_macro).reset_index(drop=True)], axis=1)\n",
    "                    if micro:\n",
    "                        non_text_features = pd.concat([non_text_features, pd.DataFrame(X_micro).reset_index(drop=True)], axis=1)\n",
    "                    if semantic:\n",
    "                        non_text_features = pd.concat([non_text_features, pd.DataFrame(X_google).reset_index(drop=True)], axis=1)\n",
    "                    all_features = non_text_features\n",
    "                    all_features.columns = all_features.columns.astype(str)\n",
    "                \n",
    "                \n",
    "                reg_model = LinearRegression()\n",
    "                log_model = LogisticRegression(max_iter=1000)\n",
    "                \n",
    "                scoring_metrics_reg = {\n",
    "                    'mse': make_scorer(mean_squared_error),\n",
    "                    'mae': make_scorer(mean_absolute_error) \n",
    "                }\n",
    "\n",
    "                scoring_metrics_log = {\n",
    "                    'accuracy': 'accuracy',\n",
    "                    'f1': 'f1_weighted'\n",
    "                }\n",
    "\n",
    "                \n",
    "                cv_results_reg = cross_validate(reg_model, all_features, y_reg, cv=10, scoring=scoring_metrics_reg)\n",
    "                cv_results_log = cross_validate(log_model, all_features, y_class, cv=10, scoring=scoring_metrics_log)\n",
    "\n",
    "                mse = cv_results_reg['test_mse'].mean()\n",
    "                mae = cv_results_reg['test_mae'].mean()\n",
    "                accuracy = cv_results_log['test_accuracy'].mean()\n",
    "                f1 = cv_results_log['test_f1'].mean()\n",
    "\n",
    "                result_df.loc[len(result_df)] = [tfidf, macro, micro, semantic, mse, mae, f1, accuracy]\n",
    "                print([tfidf, macro, micro, semantic, mse, mae, f1, accuracy])\n",
    "                \n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b41378",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
