{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  2 of 2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b929f5606dcf>:60: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = pd.merge(merged, macro_indicators_df[indicator], left_index=True, right_index=True, how='left')\n",
      "<ipython-input-4-b929f5606dcf>:60: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = pd.merge(merged, macro_indicators_df[indicator], left_index=True, right_index=True, how='left')\n",
      "<ipython-input-4-b929f5606dcf>:60: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = pd.merge(merged, macro_indicators_df[indicator], left_index=True, right_index=True, how='left')\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import quandl\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import matplotlib as plt\n",
    "\n",
    "# Pull S&P 500 and VIX Data from Yahoo\n",
    "indices_data = yf.download('^GSPC ^VIX', start='1998-12-31', end='2023-11-01')\n",
    "\n",
    "# Create liquidity, SP500, and Volatility columns\n",
    "liquidity = pd.DataFrame(indices_data['Volume']['^GSPC'])\n",
    "SP_500 = pd.DataFrame(indices_data['Close']['^GSPC'])\n",
    "volatility = pd.DataFrame(indices_data['Close']['^VIX'])\n",
    "\n",
    "# Pull Michigan University Sentiment Data from FRED\n",
    "sent_data = web.DataReader(['UMCSENT'], 'fred', datetime(1998, 12, 31), datetime(2023, 11, 13))\n",
    "\n",
    "# Set my quandl key for pulling data\n",
    "quandl.ApiConfig.api_key = 'yC7JQzd6FsYXc_4F67ym'\n",
    "\n",
    "# Create a dictionary of the indicators to pull from quandl\n",
    "macro_indicators = {\n",
    "    '2_YR_Treasury': 'FRED/DGS2',\n",
    "    'Eff_Fed_Funds': 'FRED/DFF',\n",
    "    'Real_GDP': 'FRED/GDPC1',\n",
    "    'Core_CPI': 'FRED/CORESTICKM159SFRBATL',\n",
    "    'PCE': 'FRED/PCE',\n",
    "    'Unemployment': 'FRED/UNRATE',\n",
    "    'Savings_Rate': 'FRED/PSAVERT',\n",
    "    'Retail_Sales': 'FRED/RETAILMPCSMSA',\n",
    "    'Manufacturing_PMI': 'ISM/MAN_PMI'\n",
    "}\n",
    "\n",
    "# Create an empty dictionary of dataframes\n",
    "macro_indicators_df = {}\n",
    "\n",
    "# for each indicator\n",
    "for indicator in macro_indicators:\n",
    "    \n",
    "    # Pull the quandl data and add it to the dictionary\n",
    "    quandl_code = macro_indicators[indicator]\n",
    "    data = quandl.get(quandl_code)\n",
    "    macro_indicators_df[indicator] = data\n",
    "\n",
    "# Add data pulled from Fred and Yahoo to dictionary\n",
    "macro_indicators_df['Consumer_Sent'] = sent_data\n",
    "macro_indicators_df['Liquidity'] = liquidity\n",
    "macro_indicators_df['Volatility'] = volatility\n",
    "macro_indicators_df['SP_500'] = SP_500\n",
    "\n",
    "# Merge all of the data into one df\n",
    "merged = pd.DataFrame(index= pd.date_range(start=datetime(1998,12,31), end=datetime.now(), freq='D'))\n",
    "for indicator in macro_indicators_df.keys():\n",
    "    merged = pd.merge(merged, macro_indicators_df[indicator], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Impute the empty values with the most recent value, calculate pct change from 1 yr ago\n",
    "merged.columns = list(macro_indicators_df.keys())\n",
    "for col in macro_indicators_df.keys():\n",
    "    merged[col] = merged[col].ffill()\n",
    "    merged[col+\"_pct_chng\"] = merged[col].pct_change(periods=365)\n",
    "\n",
    "# Make the data frame from 12/31/1999 to present\n",
    "merged = merged[merged.index>=datetime(1999,12,31)]\n",
    "\n",
    "# Write the csv to the current directory\n",
    "merged.to_csv(os.getcwd() + \"/macro_indicators.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the columns\n",
    "macro_cols = list(macro_indicators_df.keys())\n",
    "\n",
    "# Filter for only pct change columns\n",
    "pct_cols = []\n",
    "for col in macro_indicators_df.keys():\n",
    "    if \"2_YR\" in col or \"Fed_Funds\" in col:\n",
    "        pct_cols.append(col)\n",
    "    else:\n",
    "        pct_cols.append(col + '_pct_chng')\n",
    "\n",
    "# Select specific columns\n",
    "macro = merged[macro_cols]\n",
    "pct = merged[pct_cols]\n",
    "\n",
    "# Create correlation matrices\n",
    "macro_corr = macro.corr()\n",
    "pct_corr = pct.corr()\n",
    "\n",
    "# Write correlation matrices to current directory\n",
    "macro_corr.to_csv(os.getcwd() + \"/macro_indicators_corr.csv\")\n",
    "pct_corr.to_csv(os.getcwd() + \"/pct_chng_corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the macro data\n",
    "micro_and_macro = merged.copy()\n",
    "micro_and_macro = micro_and_macro[['Eff_Fed_Funds']]\n",
    "\n",
    "# Read in company sentiment data\n",
    "folder_path = os.path.join(os.getcwd(), 'company_sentiment_count')\n",
    "dow_ciks = pd.read_excel(\"Dow_ciks.xlsx\", sheet_name='Sheet1')\n",
    "companies = list(dow_ciks['Name'])\n",
    "\n",
    "company_sentiment_dfs = {}\n",
    "\n",
    "for company in companies:\n",
    "    cleaned_company = re.sub(r'[^a-zA-Z0-9\\s]', '', company)\n",
    "    company_name = cleaned_company.replace(' ', '_')\n",
    "    file_name = company_name + '.csv'\n",
    "    \n",
    "    df = pd.read_csv(folder_path+'/'+file_name)\n",
    "    \n",
    "    col_name = company_name + '_sentiment_score'\n",
    "    df[col_name] = df['sentiment_score']\n",
    "    df.index = pd.to_datetime(df['date'])\n",
    "    \n",
    "    if \"Goldman\" in company or \"JPMorgan\" in company:\n",
    "        continue\n",
    "    \n",
    "    micro_and_macro = pd.merge(micro_and_macro, df[col_name], left_index=True, right_index=True, how='left')\n",
    "    micro_and_macro[col_name] = micro_and_macro[col_name].ffill()\n",
    "\n",
    "\n",
    "cols = micro_and_macro.columns[1:]\n",
    "\n",
    "micro_and_macro['total_sentiment'] = micro_and_macro[cols].sum(axis=1)\n",
    "micro_and_macro['total_sentiment_count'] = micro_and_macro[cols].count(axis=1)\n",
    "\n",
    "micro_and_macro['date'] = micro_and_macro.index\n",
    "micro_and_macro.to_csv(os.getcwd() + \"/micro_and_macro.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
